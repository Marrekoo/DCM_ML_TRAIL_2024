{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Choice Analysis: micro-econometrics and machine learning approaches\n",
    "\n",
    "## `Lab session 1: Artificial Neural Networks`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**February 2024**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Gabriel Nova <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Illustrate how models and theory discussed in the classroom work out in practice.\n",
    "* Help you gather hands-on modelling and data analysis skills.\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Workspace set-up`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment, to install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Google Colab**<br>\n",
    "Uncomment the following cell if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/DCM-ML-course-TRAIL/DCM_ML_TRAIL_2024\n",
    "#!pip install -r DCM_ML_TRAIL_2024/requirements_colab.txt\n",
    "#!mv \"/content/DCM_ML_TRAIL_2024/Lab sessions/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Application: Modelling mode choices` <br>\n",
    "In this lab session, we will analyse mode choice behaviour. Understanding people's preferences over transport attributes is crucial for city planners when they (re)develop  devise policies to enhance sustainable urban mobility. During this lab session, you will apply discrete choice models to uncover people's preferences over attributes, such as the travel time, travel cost, access time, and level of service. Also, you will explore whether preferences interact with covariates such as female, business, income. While doing so, you will test various utility specifications and interpret the modelling outcomes of discrete choice models.\n",
    "\n",
    "For this study, we use data from a Stated Choice (SC) experiment provided by [Apollo](https://www.apollochoicemodelling.com/examples.html)\n",
    "\n",
    "**`Learning objectives lab session 01`**\n",
    "\n",
    "After completing the following exercises you will be able to: <br>\n",
    "* Prepare (choice) data for training Artificial Neural Networks\n",
    "* Train MultiLayerPerceptron (MLP) - a praticular type of neural network - for a classification task<br>\n",
    "* Tune the hyperparameter and network architectures to improve the model performance<br>\n",
    "* Assess the performance of of competing models, based on various performance measures, including confusion matrices, and Precision, Recall, F1 scores and Matthew's coefficient<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`This lab consists of 6 parts`**\n",
    "1. Preparing (choice) data for training Artificial Neural Networks\n",
    "2. Training a MultiLayerPerceptron (MLP) neural network\n",
    "3. Using Early stopping to avoid overfitting\n",
    "4. Using k-fold cross validation to evaluate generalisation performance\n",
    "5. Tuning hyperparameter\n",
    "6. Evaluating performance of trained models\n",
    "\n",
    "and comprises **`7`** exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Import packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will import all the Python libraries that we will use in this lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Import selected functions and classes from Python packages\n",
    "from os import getcwd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, log_loss, matthews_corrcoef, make_scorer, classification_report\n",
    "\n",
    "# Biogeme\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta, Variable, log\n",
    "\n",
    "# Setting\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `0. Preparing (choice) data for training Artificial Neural Networks`\n",
    "To prepare the data set, we will:<br>\n",
    "    1.1 **Load** the data set<br>\n",
    "    1.2 **Inspect** and **Clean** the data set<br>\n",
    "    1.3 **Discover and visualise** the data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `i. Set up the workspace and load the database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "data_path =  Path(f'data/apollo_modeChoiceData.csv')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice data into a pandas DataFrame\n",
    "df = pd.read_csv(data_path,sep = ',')\n",
    "df = df[df['SP']==1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect and clean the data**<br>\n",
    "Before starting to analyse your data, make sure you understand what features are in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of variables**<br>\n",
    "\n",
    "Therefore, it is highly recommended to look at the description of the data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable       | Description                                           | Values                                                |\n",
    "|----------------|-------------------------------------------------------|-------------------------------------------------------|\n",
    "| ID             | Unique individual ID                                  | 1 to 500                                              |\n",
    "| RP             | RP data identifier                                   | 1 for RP, 0 for SP                                    |\n",
    "| SP             | SP data identifier                                   | 1 for SP, 0 for RP                                    |\n",
    "| RP_journey     | Index for RP observations                             | 1 to 2, NA for SP                                     |\n",
    "| SP_task        | Index for SP observations                             | 1 to 14, NA for RP                                    |\n",
    "| av_car         | Availability for alternative 1 (car)                 | 1 for available, 0 for unavailable                    |\n",
    "| av_bus         | Availability for alternative 2 (bus)                 | 1 for available, 0 for unavailable                    |\n",
    "| av_air         | Availability for alternative 3 (air)                 | 1 for available, 0 for unavailable                    |\n",
    "| av_rail        | Availability for alternative 4 (rail)                | 1 for available, 0 for unavailable                    |\n",
    "| time_car       | Travel time (mins) for alternative 1 (car)           | Min: 250, mean: 311.79, max: 390 (0 if not available) |\n",
    "| cost_car       | Travel cost (£) for alternative 1 (car)              | Min: 30, mean: 39.99, max: 50 (0 if not available)    |\n",
    "| time_bus       | Travel time (mins) for alternative 2 (bus)           | Min: 300, mean: 370.29, max: 420 (0 if not available) |\n",
    "| cost_bus       | Travel cost (£) for alternative 2 (bus)              | Min: 15, mean: 25.02, max: 35 (0 if not available)    |\n",
    "| access_bus     | Access time (mins) for alternative 2 (bus)           | Min: 5, mean: 15.02, max: 25 (0 if not available)      |\n",
    "| time_air       | Travel time (mins) for alternative 3 (air)           | Min: 50, mean: 70.07, max: 90 (0 if not available)    |\n",
    "| cost_air       | Travel cost (£) for alternative 3 (air)              | Min: 50, mean: 79.94, max: 110 (0 if not available)   |\n",
    "| access_air     | Access time (mins) for alternative 3 (air)           | Min: 35, mean: 45.02, max: 55 (0 if not available)    |\n",
    "| service_air    | Service quality for alternative 3 (air)              | 1 for no-frills, 2 for wifi, 3 for food (0 if not used, RP data) |\n",
    "| time_rail      | Travel time (mins) for alternative 4 (rail)          | Min: 120, mean: 142.93, max: 170 (0 if not available) |\n",
    "| cost_rail      | Travel cost (£) for alternative 4 (rail)             | Min: 35, mean: 55.03, max: 75 (0 if not available)    |\n",
    "| access_rail    | Access time (mins) for alternative 4 (rail)          | Min: 5, mean: 14.96, max: 25 (0 if not available)      |\n",
    "| service_rail   | Service quality for alternative 4 (rail)             | 1 for no-frills, 2 for wifi, 3 for food (0 if not used, RP data) |\n",
    "| female         | Dummy variable for female individuals                | 1 for female, 0 otherwise                            |\n",
    "| business       | Dummy variable for business trips                    | 1 for business trips, 0 otherwise                     |\n",
    "| income         | Income variable (£ per annum)                        | Min: 15,490, mean: 44,748.27, max: 74,891              |\n",
    "| choice         | Choice variable                                     | 1 for car, 2 for bus, 3 for air, 4 for rail            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Descriptive statistics`**<br>\n",
    "\n",
    "We can use `describe()` to view descriptive statistics, such as count, mean, std, min, percentiles, and max about the **attribute levels** of the alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# list of relevant features\n",
    "id         = ['ID', 'SP_task' ] \n",
    "\n",
    "# Attributes\n",
    "attributes =   ['time_car',  'cost_car', \n",
    "                'time_bus',  'cost_bus',  'access_bus',\n",
    "                'time_air',  'cost_air',  'access_air',  'service_air', \n",
    "                'time_rail', 'cost_rail', 'access_rail', 'service_rail']\n",
    "\n",
    "# Socio-demographic variables\n",
    "sociovars = ['female', 'business','income']\n",
    "\n",
    "# Availability variables\n",
    "av = ['av_car', 'av_bus', 'av_rail', 'av_air']\n",
    "\n",
    "# Create a new instance of the dataframe, with the atrtributes, socio-demographic variables and the choice\n",
    "dff = df[id + attributes + sociovars + av +['choice'] ].copy()\n",
    "\n",
    "print(dff.shape)\n",
    "dff\n",
    "\n",
    "# Inspect the data types in the df\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discovering and visualising the data**<br>\n",
    "Before starting to analyse your data with models, it is advisable to start with some **descriptive analyses**.<br>\n",
    "Therefore is recommended to look first at the distribution and correlations of key feature in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 1: Is the data set (im)balanced?``\n",
    "`A` Create a histogram showing how often Car, Bus, Air, and Rail are chosen. Do not forget to add labels to the columns<br>\n",
    "`B` Interpret the the histogram. Is the data set imbalanced? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A) \n",
    "# B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 2: Explore correlations between features``\n",
    "`A` Create a heatmap to identify what features particulalrly correlate with the **CHOICE**<br>\n",
    "`B` Identify the features that strongly correlate (corr >0.9) (if any). Do they make sense? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A) \n",
    "# B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1. Estimate benchmark discrete choice model`<br>\n",
    "As a benchmark, we use the linear-additive RUM model model. We estimate this model using `biogeme`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dff into biogeme database\n",
    "biodata = db.Database('Apollo_mode_choice_data', dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Biogeme variables\n",
    "\n",
    "# Attributes of alternative 1\n",
    "time_car     = Variable('time_car')\n",
    "cost_car     = Variable('cost_car')\n",
    "\n",
    "# Attributes of alternative 2    \n",
    "time_bus     = Variable('time_bus')\n",
    "cost_bus     = Variable('cost_bus')\n",
    "access_bus   = Variable('access_bus')\n",
    "    \n",
    "# Attributes of alternative 3\n",
    "time_air     = Variable('time_air')\n",
    "cost_air     = Variable('cost_air')\n",
    "access_air   = Variable('access_air')\n",
    "service_air  = Variable('service_air')\n",
    "\n",
    "# Attributes of alternative 4\n",
    "time_rail     = Variable('time_rail')\n",
    "cost_rail     = Variable('cost_rail')\n",
    "access_rail   = Variable('access_rail')\n",
    "service_rail  = Variable('service_rail')\n",
    "\n",
    "# Availability variables\n",
    "av_car       = Variable('av_car')\n",
    "av_bus       = Variable('av_bus')\n",
    "av_rail      = Variable('av_rail')\n",
    "av_air       = Variable('av_air')\n",
    "\n",
    "# choice\n",
    "choice      = Variable('choice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the model    \n",
    "model_name = 'Linear-additive RUM-MNL'\n",
    "\n",
    "# Define the model parameters, using the function \"Beta()\", in which you must define:\n",
    "B_time        = Beta('B_time',      0, None, None, 0)\n",
    "B_cost        = Beta('B_cost',      0, None, None, 0)\n",
    "B_access      = Beta('B_access',    0, None, None, 0)\n",
    "B_service     = Beta('B_service',   0, None, None, 0)\n",
    "ASC_car       = Beta('ASC_car',     0, None, None, 0)\n",
    "ASC_bus       = Beta('ASC_bus',     0, None, None, 0)\n",
    "ASC_air       = Beta('ASC_air',     0, None, None, 0)\n",
    "ASC_rail      = Beta('ASC_rail',    0, None, None, 0)\n",
    "\n",
    "# Define the utility functions\n",
    "V1 = ASC_car +  B_time * time_car   + B_cost * cost_car\n",
    "V2 = ASC_bus +  B_time * time_bus   + B_cost * cost_bus  + B_access * access_bus\n",
    "V3 = ASC_air +  B_time * time_air   + B_cost * cost_air  + B_access * access_air  + B_service * service_air\n",
    "V4 = ASC_rail + B_time * time_rail + B_cost * cost_rail + B_access * access_rail + B_service * service_rail\n",
    "\n",
    "# Associate utility functions with alternatives\n",
    "V = {1: V1, 2: V2, 3: V3, 4: V4}    \n",
    "\n",
    "# Associate the availability conditions with the alternatives\n",
    "AV = {1: av_car, 2: av_bus, 3: av_air, 4: av_rail} \n",
    "\n",
    "# Definition of the model. This is the contribution of each observation to the log likelihood function.\n",
    "prob = models.logit(V, AV, choice)\n",
    "\n",
    "# Create the Biogeme object\n",
    "biogeme = bio.BIOGEME(biodata, log(prob))\n",
    "\n",
    "# Set reporting levels\n",
    "biogeme.generate_pickle = False\n",
    "biogeme.generate_html = False\n",
    "biogeme.saveIterations = False\n",
    "biogeme.modelName = model_name\n",
    "\n",
    "# Compute the null loglikelihood for reporting\n",
    "biogeme.calculateNullLoglikelihood(AV)\n",
    "\n",
    "# Estimate the parameters\n",
    "results_rum_mnl = biogeme.estimate()\n",
    "print(results_rum_mnl.short_summary())\n",
    "\n",
    "# Get the results in a pandas table\n",
    "beta_hat = results_rum_mnl.getEstimatedParameters()\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2. Training a MultiLayerPerceptron (MLP) neural network`\n",
    "To train Artifical Neural Networks we take the following steps:<br>\n",
    "    2.1 **Scaling** the features<br>\n",
    "    2.2 **Splitting** the data in a training and test data set<br>\n",
    "    2.3 **Creating MLP** object<br>\n",
    "    2.4 **Training the MLP** on the train data<br>\n",
    "    2.5 **Evaluating** the performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i. Scaling the features`<br>\n",
    "To efficiently train ANNs it strongly recommended to **scale** (a.k.a. normalise) the features. There are several ways to scale your data. A commonly used scaler of `sk-learn` is called 'StandardScaler'. This scaler normalises the variance and shift the location of the distribution to zero, see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the list of features that need to be scaled\n",
    "# Importantly, this excludes the availabilities and the choice\n",
    "features2scale = attributes + sociovars \n",
    "\n",
    "# Initiate scaler object & fit to data\n",
    "scaler = StandardScaler()  \n",
    "dff_scaled_f = scaler.fit_transform(dff.loc[:,features2scale]) \n",
    "dff_scaled_f = pd.DataFrame(dff_scaled_f, columns = features2scale) \n",
    "dff_scaled_f.reset_index(drop=True, inplace=True)  # Reset indices\n",
    "\n",
    "# Create new dataframe X_scaled containingg the scaled features AND the (unscaled) ones: Group, Survey, Choice\n",
    "dff_scaled = dff.copy()\n",
    "dff_scaled.reset_index(drop=True, inplace=True)\n",
    "dff_scaled[features2scale] = dff_scaled_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ii. Splitting the data in a train set and a test set`<br>\n",
    "Training ML models always involves a **train** and a **test** data set. The train set is used to update the weights of the model. Tha test set is used to evaluate the **generalisation performance** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of features that we want to use in the model\n",
    "features = attributes + sociovars + av\n",
    "\n",
    "# Create the target\n",
    "Y = dff_scaled['choice']\n",
    "\n",
    "# Split the data using sk-learn's `train_test_split` function\n",
    "# Note that we use 60% for training and 40% for testing\n",
    "# Note that we set the random_state, in order to replicate results later (do not change) \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dff_scaled[features], Y, random_state = 12345, test_size = 0.4)\n",
    "\n",
    "# Store the indices of test sets for later use\n",
    "test_indices = X_test.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iii. Creating the MLP object`<br>\n",
    "A MultiLayerPerceptron (MLP) is a fully-connected feed-forward neural network. We create the MLP using `sk-learn's` MLPClassifier function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'plain vanilla' MLP object\n",
    "# Declare the number of layers and nodes per layer\n",
    "# layers = (a,b) means two layer with a nodes in the 1st hidden layer and b nodes in the 2nd hidden layer\n",
    "layers = (10)\n",
    "\n",
    "# Define MLP architecture, optimiser and hyperparameters:\n",
    "# We use Adam optimiser\n",
    "# We use the learning rate to 0.001\n",
    "# We use L2 regularisation of 0\n",
    "# We use a batch size of 250 observations\n",
    "# We use relu activation (transfer function)\n",
    "# We set the max number of epochs to 2000\n",
    "mlp = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0, batch_size=250, activation = 'relu', max_iter = 2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iv. Training the MLP`<br>\n",
    "To train the MLP we use the '.fit' function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP using the train data.\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to look at the learning curve, to see how much the model has improved from the initial point,\n",
    "# and how is has evolved over epochs.\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.grid(True,linewidth = 0.5)\n",
    "ax.set_ylim(0.6,1.2)\n",
    "ax.set_xlim(0,mlp.n_iter_)\n",
    "ax.set_title(f'Cross entropy loss on the TRAINING DATA. \\nBest CE = {mlp.loss_:4.3f}')\n",
    "# The plot illustrates that the training stops when the training loss does no longer improve more that a given tolerance (e.g. 1e-6), or reaches max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`v. Evaluating the model performance` <br>\n",
    "Machine learning researchers usually evaluate the performance of a classification model using the cross-entropy. A lower cross entrop indicates a better model. In contrast, choice modellers usually look at the Log-likelihood (LL) and the rho-square. A high LL and rho-square indicate a better model. <br>\n",
    "As there is no standard function that outputs the evaluation metrics of both disciplines, below we create our **own evaluation function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an eavluation function that returns key evaluation metrics: LL, LL0, cross_entropy, rho_sq\n",
    "# To compute these performance metrics, the function takes as inputs:\n",
    "#   - the predicted probabilities (prob)\n",
    "#   - the choices (Y)\n",
    "#   - the availabilities (AV)\n",
    "def eval_performance(prob,Y,AV):\n",
    "    \n",
    "    # Calculate the likelihood of the data given the model\n",
    "    LL = np.sum(np.log(np.sum(prob*Y,axis=1)))\n",
    "\n",
    "    # Calculate the Null-loglikelihood\n",
    "    LL0 = np.sum(np.log(np.divide(1,np.sum(AV,axis=1))))\n",
    "\n",
    "    # Calculate cross-entropy\n",
    "    cross_entropy =  -LL/len(AV)\n",
    "    \n",
    "    # Calculate the rho_sq\n",
    "    rho_sq = 1 -(LL/LL0)\n",
    "    return LL, LL0, cross_entropy, rho_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function to evaluate and compare the performance of our MLP on the training and test data sets\n",
    "eval_train = eval_performance(mlp.predict_proba(X_train),np.transpose([Y_train ==1,Y_train ==2,Y_train ==3, Y_train ==4]), X_train[['av_car','av_bus','av_rail','av_air']])\n",
    "eval_test  = eval_performance(mlp.predict_proba(X_test), np.transpose([Y_test  ==1,Y_test  ==2,Y_test  ==3, Y_test  ==4]), X_test[['av_car','av_bus','av_rail','av_air']])\n",
    "\n",
    "# Print the results\n",
    "print('Model performance of the plain vanilla MLP:')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results = pd.DataFrame({'data set':     ['Train','Test'],\n",
    "                        'LL':           [eval_train[0], eval_test[0]],\n",
    "                        'LL0':          [eval_train[1], eval_test[1]],\n",
    "                        'cross_entropy':[eval_train[2], eval_test[2]],\n",
    "                        'rho_sq':       [eval_train[3], eval_test[3]]})\n",
    "print(results.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how well this plain vanilla MLP performs, below are the same performance metrics for a RUM-MNL model printed (on all data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LL\\t\\t {results_rum_mnl.getGeneralStatistics()[\"Final log likelihood\"][0]:0.2f} \\\n",
    "      \\nLL0\\t\\t {results_rum_mnl.getGeneralStatistics()[\"Null log likelihood\"][0]:0.2f} \\\n",
    "      \\ncross_entropy\\t {-results_rum_mnl.getGeneralStatistics()[\"Final log likelihood\"][0]/len(dff):0.2f} \\\n",
    "      \\nrho_sq\\t\\t {results_rum_mnl.getGeneralStatistics()[\"Rho-square for the null model\"][0]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As can be seen, the model fit of the MLP is considerably higher than of the linear-additive RUM-MNL model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 3: Does using more nodes improve the model performance?``\n",
    "`A` Calculate the number of weights consumed by the current MLP with 10 hidden nodes in 1 hidden layer. <br>\n",
    "`B` Retrain your model several times with {10,30,60,90} nodes. Report the cross-entropy performance on the train and test data sets.<br>\n",
    "`C` Does increasing the number of nodes lead to a lower cross-entropy on the train and or test set? What is happening?<br>\n",
    "`D` Suppose you would have many more choice observations from this survey. Would that enable you to develop a much better model with a cross entropy performance of say <0.10 (on the test set)? Explain your answer. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A)\n",
    "# B)\n",
    "# C)\n",
    "# D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3. Using Early stopping to avoid overfitting`\n",
    "Early stopping refers to a technique that stops the training of the network when the performance on the test data set no longer improves. Thereby, early stopping avoids the model to overfit the data. It essetially stops the training before the model can overfit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is the same 'plain vanilla' MLP, but now we set early_stopping = True\n",
    "# A validation_fraction is added. This fraction is the proportion of training data to set aside as validation set for early stopping. This data set is used to determine when to stop. \n",
    "# The training stops when the performance on the validation data set does not improve for n_iter_no_change in a row\n",
    "# We use the MLP with 10 nodes and one hidden layer again.\n",
    "layers = (10)\n",
    "n_iter_no_change = 10\n",
    "mlp_early_st = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0, batch_size=250, activation = 'relu', max_iter = 2000, early_stopping=True, n_iter_no_change = n_iter_no_change,validation_fraction = 0.25) \n",
    "\n",
    "# Train the MLP using the train data\n",
    "# Note that we use df.values here. This is due to a small bug in sk-learn. Without .values sk-learn still works, but prompts some warnings\n",
    "mlp_early_st.fit(X_train.values, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to look at the learning curve, to see how much the model has improved from the initial point,\n",
    "# and how it has evolved over the epochs.\n",
    "# The learning curve plot also illustrates well what early stopping does.\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "plt.plot(mlp.loss_curve_,label=f'MLP with {mlp.hidden_layer_sizes} hidden nodes, WITHOUT early stopping')\n",
    "plt.plot(mlp_early_st.loss_curve_,label=f'MLP with {mlp_early_st.hidden_layer_sizes} hidden nodes, WITH early stopping')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.grid(True,linewidth = 0.5)\n",
    "ax.set_ylim(0.6,1.2)\n",
    "ax.set_xlim(0,mlp.n_iter_)\n",
    "ax.set_title('Effect of early stopping')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our evaluation function again to evaluate performance of the MLP with early stopping on the training and test data set\n",
    "eval_train_early_st = eval_performance(mlp_early_st.predict_proba(X_train.values),np.transpose([Y_train ==1,Y_train ==2,Y_train ==3, Y_train ==4]), X_train[['av_car','av_bus','av_rail','av_air']])\n",
    "eval_test_early_st  = eval_performance(mlp_early_st.predict_proba(X_test.values), np.transpose([Y_test  ==1,Y_test  ==2,Y_test  ==3, Y_test  ==4]), X_test[['av_car','av_bus','av_rail','av_air']])\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results_early_st = pd.DataFrame({'data set':     ['Train','Test'],\n",
    "                        'LL':           [eval_train_early_st[0], eval_test_early_st[0]],\n",
    "                        'LL0':          [eval_train_early_st[1], eval_test_early_st[1]],\n",
    "                        'cross_entropy':[eval_train_early_st[2], eval_test_early_st[2]],\n",
    "                        'rho_sq':       [eval_train_early_st[3], eval_test_early_st[3]]})\n",
    "print(results_early_st.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 4: Can we overfit with early stopping?``\n",
    "`A` Did early stopping reduced overfitting? How can you see this from the printed results?<br>\n",
    "`B` Try if early stoppping also helps to avoid overfitting when using more nodes, e.g. 100, or (50,50) nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A)\n",
    "# B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `4. Using k-fold cross validation to evaluate generalisation performance`\n",
    "k-fold cross validation is commonly used to more accurately **evaluate the generalisation performance** of a given network. It improves a simple train-test split approach in that it systematically cuts the data set in k pieces. k-fold cross validation is especially crucial when tuning hyperparameters (as we will see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP object (plain vanilla MLP without early stopping)\n",
    "layers = 10\n",
    "mlp_cv = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0, batch_size=250, activation = 'relu', max_iter = 2000) \n",
    "\n",
    "# Create scoring function\n",
    "# It is necessary to create a scoring function when working with cross_validate of sk-learn\n",
    "# We set `greater_is_better` to `False` as we are minimising cross entropy loss\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Apply cross_validate, using e.g. 5 folds\n",
    "# Since we use cross-validation training takes n_folds times longer than using a train-test split\n",
    "n_folds = 5\n",
    "cv_results = cross_validate(mlp_cv,dff_scaled[features],Y, cv = n_folds, scoring=logloss,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and test performance in a bar plot, for each fold\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_axis = np.arange(n_folds)\n",
    "ax.bar(x_axis + -0.125, -cv_results['train_score'], color = 'b', width = 0.25,label = 'Train data set')\n",
    "ax.bar(x_axis +  0.125, -cv_results['test_score'], color = 'g', width = 0.25,label = 'Test data set')\n",
    "ax.set_xlabel('fold #')\n",
    "ax.set_ylabel('cross entropy')\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hence, this cross validation analysis supports the finding that a plain vanilla MLP (i.e. without early stopping) `modestly` overfits the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `5. Tuning hyperparameter`\n",
    "When training MLPs (and most other ML models) there are several parameters we can **'tune'** (optimise) to improve the model's performance. The process of doing this is called **hyperparameter tuning**. Hyperparameter tuning can be done manually, but that is unwieldy. The GridSearchCV function in `sk-learn` automates the hyperparemeter tuning process. When tuning the hyperparameters, it is mandatory to use a k-fold cross validation approach. Otherwise, there is a risk of overfitting on the test set *because* the parameters can be tweaked until the estimator performs optimally on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change these lines (they ensure the code runs fast enough on colab and local, by reducing the no. iterations for colab)\n",
    "try:\n",
    "    num_iters \n",
    "except NameError:\n",
    "    num_iters = 2000\n",
    "else:\n",
    "    num_iters = 250\n",
    "\n",
    "# Create MLP object (plain vanilla MLP)\n",
    "mlp_gs = MLPClassifier(activation = 'relu', solver='adam', batch_size=250, max_iter=num_iters)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "# 'hidden_layer_sizes' defines the number of nodes and layers\n",
    "# 'alpha' governs the L2 regularisation\n",
    "# 'learning_rate_init' governs the learning rate.\n",
    "hyperparameter_space = {\n",
    "    'hidden_layer_sizes': [(10),(10,10),(30,30)],\n",
    "    'alpha': [0,0.01,1],\n",
    "    'learning_rate_init': [0.01,0.001,0.0001]}\n",
    "\n",
    "# Create scoring function\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Create the grid_search object, with using the MLP classifier\n",
    "folds = 5 # Number of cross validation splits\n",
    "mlp_gridsearch = GridSearchCV(mlp_gs, hyperparameter_space, n_jobs=-1, cv=folds,scoring = logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the training/gridsearch\n",
    "# Note that this is computationally expensive! \n",
    "# It may take up to 5 minutes, since 3 x 3 x 3 = 27 models need to be trained, each with 5 folds (=135)\n",
    "mlp_gridsearch.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your hyperparameter tuning results, so we only have to do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model \n",
    "filename = 'my_tuned_model.sav'\n",
    "pickle.dump(mlp_gridsearch, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model (if you have a saved model)\n",
    "# mlp_gridsearch = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results into a new pandas dataframe\n",
    "df_gridsearch = pd.DataFrame.from_dict(mlp_gridsearch.cv_results_)\n",
    "    \n",
    "# Add new column with a label for the hyperparameter combinations %% NOT SURE IF THIS IS BET WAY TO DO THIS IN PYTHON   \n",
    "df_gridsearch['gs_combinations'] = 'L2 = '+ df_gridsearch['param_alpha'].astype('str') + '; Learning_rate = '+ df_gridsearch['param_learning_rate_init'].astype('str') + '; Layers = ' + df_gridsearch['param_hidden_layer_sizes'].astype('str')\n",
    "df_gridsearch = df_gridsearch.sort_values('rank_test_score')\n",
    "\n",
    "# Visualise deviation in performance across hyper parameter settings\n",
    "plt.figure(figsize = (16,6))\n",
    "ax = sns.barplot(x = df_gridsearch.gs_combinations,y=-df_gridsearch.mean_test_score,palette=\"Blues_d\",)\n",
    "ax.set_ylim(0.6,1.2)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "print('Best hyperparameters found:\\t', mlp_gridsearch.best_params_)\n",
    "print('Best model performance:\\t\\t', -mlp_gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 5: Hyperparameter tuning``\n",
    "`A` Compare the performance of the hypertuned model with the plain vanilla MLP. Has the generalisation performance improved? How much?<br>\n",
    "`B` What hyperparameter turns out to be particularly impactful on the model performance?<br>\n",
    "`C` Can you think of reasons why this could be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A)\n",
    "# B)\n",
    "# C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Re)Training the model with optimised hyperparameters**<br> \n",
    "After completing hypertuning, you know the optimal hyperparameters. <br>\n",
    "Therefore, after hypertuning we always retrain the model using the optimised hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new mlp object using the optimised hyperparameters, just using the train/test split\n",
    "layers = mlp_gridsearch.best_params_['hidden_layer_sizes']\n",
    "lr = mlp_gridsearch.best_params_['learning_rate_init']\n",
    "alpha = mlp_gridsearch.best_params_['alpha']\n",
    "mlp_gs = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = lr, alpha=alpha, batch_size=250, activation = 'tanh', max_iter = 2000) \n",
    "\n",
    "# Train the model\n",
    "mlp_gs.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also evaluate performance on the hypertuned model using our evaluation function\n",
    "# Note we use the full data here\n",
    "eval_gridsearch = eval_performance(mlp_gs.predict_proba(dff_scaled[features]),np.transpose([Y ==1,Y ==2,Y ==3, Y==4]), dff_scaled[['av_car','av_bus','av_rail','av_air']])\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results_clf = pd.DataFrame({'data set': ['All data'],\n",
    "                        'LL':           [eval_gridsearch[0]],\n",
    "                        'LL0':          [eval_gridsearch[1]],\n",
    "                        'cross_entropy':[eval_gridsearch[2]],\n",
    "                        'rho_sq':       [eval_gridsearch[3]]})\n",
    "print(results_clf.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `6. Evaluating and comparing performances across trained models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of a wide range is metrics are available, beyond generalisation performance.<br>\n",
    "Here, we look at:<br>\n",
    "i. Confusion matrix<br>\n",
    "ii. Precision, Recall, and F1-score<br>\n",
    "iii. Matthew's correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Confusion matrix**<br>\n",
    "Confusion matrices shows counts from predicted and actual outcomes. The counts on the diagonal are correctly classified outcomes (the model predictions and the ground true are the same). The counts on the off diagonal elements are the misclassified outcomes. Hence, the best classifier will have a confusion matrix with only diagonal elements and the rest of the elements set to zero.<br>\n",
    "We compare the **RUM-MNL** against the **MLP with hyperparameter tuning**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to simulate the choice probabilities for each alternative for each observation in the test dataset using biogeme\n",
    "# Using these, we can get the 0/1 predictions (based on the highest probability)\n",
    "biodata_sim = db.Database('biodata_sim_test', dff.iloc[test_indices,:])\n",
    "prob_car  = models.logit(V, AV, 1)\n",
    "prob_bus  = models.logit(V, AV, 2)\n",
    "prob_air  = models.logit(V, AV, 3)\n",
    "prob_rail = models.logit(V, AV, 4)\n",
    "\n",
    "# Create a dictionary with the choice probabilities\n",
    "simulate = {\n",
    "    'Prob. car': prob_car,\n",
    "    'Prob. bus': prob_bus,\n",
    "    'Prob. air': prob_air,\n",
    "    'Prob. rail': prob_rail}\n",
    "\n",
    "# Simulate the choice probabilities\n",
    "biogeme_sim = bio.BIOGEME(biodata_sim, simulate)\n",
    "simulated_values = biogeme_sim.simulate(results_rum_mnl.getBetaValues())\n",
    "\n",
    "# Get the 0/1 predictions (based on the highest probability)\n",
    "Y_pred_mnl = np.argmax(simulated_values.values,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the choices for the test data set, using the MLP trained with early stopping and MLP with hyperparameters tuned\n",
    "Y_pred_gs = mlp_gs.predict(X_test)                      # 0/1 predictions of MLP with hyperparameters tuned\n",
    "\n",
    "# Show the confusion matrices, to compare the hyperparameter tuned network with the early stopping network\n",
    "fig, ax = plt.subplots(2,2,figsize = (16,12))\n",
    "fig.set_tight_layout(True)\n",
    "ylabels = ['Car', 'Bus', 'Air', 'Rail']\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_mnl, display_labels = ylabels, normalize= None, ax=ax[(0,0)])\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_mnl, display_labels = ylabels, normalize='true',ax=ax[(1,0)])\n",
    "cm3 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_gs,  display_labels = ylabels, normalize= None, ax=ax[(0,1)])\n",
    "cm4 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_gs,  display_labels = ylabels, normalize='true',ax=ax[(1,1)])\n",
    "\n",
    "# Add titles\n",
    "cm1.ax_.set_title(f'RUM-MNL model')\n",
    "cm2.ax_.set_title(f'RUM-MNL model \\n Normalised')\n",
    "cm3.ax_.set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes / hyperparameters tuned ')\n",
    "cm4.ax_.set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes / hyperparameters tuned \\n Normalised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 6: Model accuracy``\n",
    "Accuracy is defined as the true positives over the total number of cases.<br>\n",
    "`A` Manually compute the prediction accuracy of the model with early stopping and the model with hyperparameter tuning<br>\n",
    "`B` For which class (Car,Bus,Air,Train) does the hyperparameter tuning improves the prediction accuracy most?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A)\n",
    "# B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Precision, Recall, and F1**<br>\n",
    "Looking at the confusion matrices, the improvements in prediction accuracy due to the hyperparameter tuning may not seem very impressive. However, one should keep in mind that 0/1 predictions are sensitive to class imbalances, which are present in these data. Moreover, accuracy can be a misleading metric for imbalanced data sets. A naive model that would simply always predict \"Rail\" will already do quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the model performance in more depth, we thus must look at the predictions at the level of the classes.<br> \n",
    "Next, we compute Precision, Recall, and F1 score.<br>\n",
    "* **Precision** Tells you what fraction of predictions for a given class are actually of that class.<br>\n",
    "* **Recall** Tells what fraction of all observations belonging to a given class are correctly predicted as such by the model. Recall is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. <br>\n",
    "* **F1 score** combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall. It can be calculated as follows: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the precision, recal and f1 score we conveniently use sk-learn's 'classification_report' functionality\n",
    "print('Classification report for RUM-MNL model\\n',classification_report(Y_test,Y_pred_mnl, target_names= ylabels))\n",
    "print('\\nClassification report for plain vanilla MLP with hyperparameters tuned\\n',classification_report(Y_test,Y_pred_gs, target_names= ylabels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52edd5821628e65ed257fae09420cd5b7ee9f38a64f5e5ba5a5e47a3545ddf85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
